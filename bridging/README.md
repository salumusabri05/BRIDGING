# Bridging Silence ğŸ¤Ÿ

**Tanzanian Sign Language Recognition App** - Converting sign language to text and speech using AI-powered hand landmark detection.

## âš ï¸ PROJECT STATUS - IN ACTIVE DEVELOPMENT

**Current Phase:** Hand Detection Implementation

### âœ… What Currently Works:
- Beautiful camera UI with smooth animations
- Backend API integration and communication
- Text-to-speech functionality
- User interface and navigation
- Camera permissions and controls

### ğŸš§ Under Active Development:
- **Real-time hand detection** (implementing TensorFlow Lite)
- **Live sign recognition** (pending hand detection completion)
- **Hand landmark visualization** (pending hand detection)

### ğŸ“… Development Timeline:
- **Phase 1** (Current): TensorFlow Lite integration - In Progress
- **Phase 2** (Next 2 weeks): End-to-end testing and optimization
- **Phase 3** (1 month): Beta testing with users
- **Phase 4** (6 weeks): Production release

**Note:** The app's core feature (hand detection) is being reimplemented with TensorFlow Lite for better performance and accuracy. The previous MediaPipe WebView approach had architectural limitations.

---

# Original Vision & Target Architecture

## ğŸ¯ Overview

Bridging Silence is a React Native mobile application built with Expo that will enable real-time recognition of Tanzanian Sign Language (TSL). The app will capture hand gestures through the device camera, process them using TensorFlow Lite for landmark detection, and send the data to a machine learning model for sign prediction.

## âœ¨ Planned Features

- ğŸ¥ **Real-time Video Detection** - Continuous hand tracking and recognition
- ğŸ“¸ **Live Camera Feed** - Front/back camera with real-time preview
- ğŸ¤– **AI-Powered Recognition** - MediaPipe Hands detects 21 hand landmarks per frame
- âš¡ **Frame Throttling** - Optimized processing (10 FPS) for performance and battery
- ğŸ”Š **Text-to-Speech** - Converts recognized signs to spoken words
- ğŸ“Š **Live Confidence Meter** - Visual feedback on recognition accuracy
- ğŸ“ **History Tracking** - Builds words from sequential letter signs
- ğŸ¨ **Hand Visualization** - Real-time overlay showing detected hand skeleton
- ğŸ”´ **Live Indicator** - Shows detection status and FPS counter
- ğŸŒ™ **Dark/Light Mode** - Automatic theme switching

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Camera View   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MediaPipe     â”‚ â† Detects 21 landmarks
â”‚   Hands API     â”‚   (x, y, z) Ã— 21
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Normalization  â”‚ â† Min-max scaling
â”‚   (63 features) â”‚   to [0, 1] range
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend API   â”‚ â† ML model prediction
â”‚ (TSL Classifier)â”‚   returns letter + confidence
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text-to-Speech  â”‚ â† Spoken output
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Getting Started

### Prerequisites

- Node.js 18+ 
- npm or yarn
- Expo CLI (`npm install -g expo-cli`)
- iOS Simulator or Android Emulator (or physical device with Expo Go)

### Installation

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd bridging
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Start the development server**
   ```bash
   npx expo start
   ```

4. **Run on your device**
   - Scan the QR code with Expo Go (Android/iOS)
   - Or press `a` for Android emulator
   - Or press `i` for iOS simulator

## ğŸ“± Usage

1. **Grant Camera Permission** - Allow camera access when prompted
2. **Position Your Hand** - Place your hand in front of the camera
3. **Capture Gesture** - Tap the blue capture button
4. **View Results** - See the predicted letter and confidence score
5. **Hear It Spoken** - The app automatically speaks the recognized letter
6. **Build Words** - Capture multiple letters to form words
7. **Speak Word** - Tap "Speak" to hear the complete word

## ğŸ› ï¸ Technical Stack

### Frontend
- **React Native** - Cross-platform mobile framework
- **Expo** - Development platform and SDK
- **TypeScript** - Type-safe JavaScript
- **Expo Camera** - Camera access and frame capture
- **Expo Speech** - Text-to-speech synthesis
- **react-native-svg** - Hand skeleton visualization

### Computer Vision
- **MediaPipe Hands** - Hand landmark detection
- **@mediapipe/tasks-vision** - Web-compatible MediaPipe library

### Backend Integration
- **Axios** - HTTP client for API requests
- **Backend URL**: `https://production-model.onrender.com`
- **Model**: MLP Classifier for TSL recognition

## ğŸ“‚ Project Structure

```
bridging/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ (tabs)/
â”‚   â”‚   â”œâ”€â”€ camera.tsx       # Main camera screen
â”‚   â”‚   â”œâ”€â”€ index.tsx        # Home screen
â”‚   â”‚   â”œâ”€â”€ explore.tsx      # About screen
â”‚   â”‚   â””â”€â”€ _layout.tsx      # Tab navigation
â”‚   â”œâ”€â”€ _layout.tsx          # Root layout
â”‚   â””â”€â”€ modal.tsx
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ PredictionDisplay.tsx    # Results UI
â”‚   â”œâ”€â”€ HandVisualization.tsx    # Hand skeleton overlay
â”‚   â””â”€â”€ ...
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ api.service.ts           # Backend API integration
â”‚   â”œâ”€â”€ mediapipe.service.ts     # Hand detection service
â”‚   â””â”€â”€ speech.service.ts        # Text-to-speech
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ landmark-processor.ts    # Data normalization
â”œâ”€â”€ types/
â”‚   â””â”€â”€ tsl.types.ts            # TypeScript definitions
â”œâ”€â”€ constants/
â”‚   â”œâ”€â”€ api.ts                  # API configuration
â”‚   â””â”€â”€ mediapipe.ts            # MediaPipe settings
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ instructions.md         # TSL data format spec
â”‚   â”œâ”€â”€ data.md                 # Backend API info
â”‚   â””â”€â”€ app.md                  # App overview
â””â”€â”€ package.json
```

## ğŸ”§ Configuration

### API Endpoint
Update the backend URL in `constants/api.ts`:
```typescript
export const API_CONFIG = {
  BASE_URL: 'https://production-model.onrender.com',
  ENDPOINTS: {
    PREDICT: '/predict',
  },
};
```

### MediaPipe Settings
Adjust detection thresholds in `constants/mediapipe.ts`:
```typescript
export const MEDIAPIPE_CONFIG = {
  MIN_DETECTION_CONFIDENCE: 0.7,
  MIN_TRACKING_CONFIDENCE: 0.5,
  MAX_NUM_HANDS: 1,
};
```

## ğŸ“Š Data Format

### Input (to Backend)
```json
{
  "landmarks": [
    [0.338775, 0.707677, 0.000000],  // Wrist
    [0.359596, 0.690019, -0.064400], // Thumb CMC
    // ... 19 more landmarks
  ]
}
```

### Output (from Backend)
```json
{
  "letter": "A",
  "confidence": 0.95
}
```

For detailed data specifications, see [docs/instructions.md](docs/instructions.md).

## ğŸ§ª Testing

### Run on Android
```bash
npm run android
```

### Run on iOS
```bash
npm run ios
```

### Run on Web
```bash
npm run web
```

## ğŸ› Troubleshooting

### Camera Not Working
- Ensure camera permissions are granted
- Restart the Expo development server
- Check if camera is working in another app

### Detection Issues
- Improve lighting conditions
- Keep hand fully visible in frame
- Use plain background
- Ensure hand is not too close or far from camera

### API Connection Errors
- Check internet connection
- Verify backend URL is correct
- Check backend server status

## ğŸš§ Known Current Limitations

- **Hand Detection**: Not yet fully implemented - transitioning from MediaPipe WebView to TensorFlow Lite native
- **Real-time Recognition**: Pending hand detection implementation
- **Static Signs Only**: Will initially support static gestures, dynamic/motion-based signs planned for v2.0
- **Single Hand**: Currently designed for one hand detection at a time

## ğŸ”® Future Enhancements (Post-Launch)

- [ ] Native MediaPipe integration for real-time detection
- [ ] Support for dynamic/motion-based signs
- [ ] Offline mode with local TensorFlow Lite model
- [ ] Two-hand sign recognition
- [ ] Custom sign training
- [ ] Sign language tutorials
- [ ] Social features (share signs, challenges)

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- MediaPipe team for hand landmark detection
- Expo team for the amazing development platform
- TSL community for inspiration and feedback

## ğŸ“ Support

For issues, questions, or contributions, please open an issue on GitHub.

---

**Built with â¤ï¸ for the deaf community in Tanzania**
